{
  "name": "tiny_23M_chinchilla_500M",
  "description": "Tiny 23M parameter model with Chinchilla-optimal token budget",
  "output_dir": "data/processed/tiny_23M_500M_tokens_32k_1024",
  "target_total_tokens": 500000000,
  "tokenizer_path": "data/tokenizer/spm32k",
  "vocab_size": 32768,
  "sequence_length": 1024,
  "train_ratio": 0.98,
  "shard_tokens": 2000000,
  "use_minhash": true,
  "minhash_threshold": 0.85,
  "sources": [
    {
      "name": "wikipedia_foundation",
      "config_path": "config/datasets/vietgpt_wikipedia_en.json",
      "weight": 0.25,
      "target_tokens": 125000000,
      "description": "Wikipedia EN - encyclopedic knowledge foundation"
    },
    {
      "name": "c4_web_diversity",
      "config_path": "config/datasets/c4_en.json",
      "weight": 0.25,
      "target_tokens": 125000000,
      "description": "C4 English - diverse web content for generalization"
    },
    {
      "name": "gutenberg_literature",
      "config_path": "config/datasets/gutenberg_books.json",
      "weight": 0.25,
      "target_tokens": 125000000,
      "description": "Project Gutenberg - classic literature and narrative structures"
    },
    {
      "name": "forums_conversation",
      "config_path": "config/datasets/stackexchange.json",
      "weight": 0.25,
      "target_tokens": 125000000,
      "description": "Forum discussions - conversational and technical content"
    }
  ],
  "_model_params": "23M",
  "_chinchilla_ratio": "~22:1 tokens per parameter",
  "_strategy": "Balanced single-phase training with diverse, high-quality sources",
  "_comment": "Chinchilla-optimal training for 23M model: 500M tokens (22x parameters). Balanced mix ensures exposure to encyclopedic, web, literary, and conversational content without curriculum complexity."
}