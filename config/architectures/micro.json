{
  "_description": "Micro model: ~9.4M parameters, smallest viable model maintaining head_dim=64 constraint",
  "model_name": "micro",

  "_architecture_comment": "LLaMA-like decoder-only transformer with modern optimizations",
  "n_layer": 4,
  "d_model": 128,
  "n_head": 2,
  "d_ff": 512,

  "_tokenization_comment": "SentencePiece vocabulary optimized for English text",
  "vocab_size": 32768,
  "sequence_length": 1024,

  "_training_params": "Dropout and normalization settings",
  "dropout": 0.1,
  "layer_norm_epsilon": 1e-5,

  "_architecture_details": {
    "head_dim": 64,
    "ffn_ratio": 4.0,
    "estimated_parameters": "~9.4M",
    "normalization": "RMSNorm",
    "activation": "SwiGLU",
    "position_encoding": "RoPE",
    "attention_type": "causal_self_attention",
    "bias_terms": false
  },
  "_memory_estimate": "~1GB VRAM (FP16), ~3-4GB training",
  "_target_use_cases": ["ultra_fast_prototyping", "smoke_tests", "ci_cd_validation", "architecture_testing"]
}
