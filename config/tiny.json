{
  "_description": "Tiny model: 23M parameters, optimized for rapid experimentation and prototyping",
  "model_name": "tiny",
  
  "_architecture_comment": "LLaMA-like decoder-only transformer with modern optimizations",
  "n_layer": 6,
  "d_model": 256,
  "n_head": 4,
  "d_ff": 1024,
  
  "_tokenization_comment": "SentencePiece vocabulary optimized for English text",
  "vocab_size": 32768,
  "sequence_length": 1024,
  
  "_training_params": "Dropout and normalization settings",
  "dropout": 0.1,
  "layer_norm_epsilon": 1e-5,
  
  "_architecture_details": {
    "head_dim": 64,
    "ffn_ratio": 4.0,
    "estimated_parameters": "~6M",
    "normalization": "RMSNorm",
    "activation": "SwiGLU", 
    "position_encoding": "RoPE",
    "attention_type": "causal_self_attention",
    "bias_terms": false
  },
  "_memory_estimate": "~2GB VRAM (FP16), ~6GB training",
  "_target_use_cases": ["prototyping", "code_testing", "quick_experiments"]
}