# ğŸ“ Configuration Pipeline Modulaire - Organisation

## ğŸ“‹ Structure des Dossiers

```
config/
â”œâ”€â”€ datasets/                    # ğŸŸ¢ ANCIENNE CONFIGURATION (Legacy)
â”‚   â”œâ”€â”€ c4_en.json              # Anciens configs individuels
â”‚   â”œâ”€â”€ gutenberg_books.json
â”‚   â”œâ”€â”€ stackexchange.json
â”‚   â””â”€â”€ ...                     # Autres anciens fichiers
â”‚
â”œâ”€â”€ meta_configs/               # ğŸŸ¢ ANCIENNE CONFIGURATION (Legacy)
â”‚   â”œâ”€â”€ phase_a.json           # Anciens meta-configs
â”‚   â”œâ”€â”€ tiny_23M_chinchilla.json
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ modular_pipeline/           # ğŸ”µ NOUVELLE CONFIGURATION (Modular)
    â”œâ”€â”€ README.md              # Ce fichier d'organisation
    â”œâ”€â”€ sources/               # Configurations des sources individuelles
    â”‚   â”œâ”€â”€ c4_english.json    # Source C4
    â”‚   â”œâ”€â”€ gutenberg_books.json # Source Gutenberg
    â”‚   â”œâ”€â”€ fineweb_edu.json   # Source FineWeb-Edu
    â”‚   â””â”€â”€ vietgpt_wikipedia.json # Source VietGPT Wikipedia
    â”‚
    â””â”€â”€ training_configs/       # Configurations complÃ¨tes d'entraÃ®nement
        â”œâ”€â”€ tokenizer_training_mix.json    # Config tokenizer (7M tokens)
        â”œâ”€â”€ tiny_23M_chinchilla_500M.json  # Config dataset 23M (500M tokens)
        â””â”€â”€ demo_fast.json                 # Config test rapide (100K tokens)
```

---

## ğŸ¯ Configurations Disponibles

### **1. EntraÃ®nement Tokenizer OPTIMAL**
- **Fichier**: `training_configs/tokenizer_training_mix.json`
- **Usage**: EntraÃ®ner le tokenizer SentencePiece global (optimal absolu)
- **Tokens**: 25M tokens Ã©chantillonnÃ©s (4 sources Ã©quilibrÃ©es)
- **Distribution**: C4(7M), VietGPT(6M), FineWeb(6M), Gutenberg(6M)
- **DurÃ©e**: ~45-60 minutes
- **RÃ©utilisable**: 23M ET 124M modÃ¨les

### **2. Dataset Tiny 23M Chinchilla**
- **Fichier**: `training_configs/tiny_23M_chinchilla_500M.json`
- **Usage**: Dataset complet pour modÃ¨le 23M paramÃ¨tres
- **Tokens**: 500M tokens (Chinchilla-optimal 22:1)
- **Sources**: C4, Gutenberg, FineWeb-Edu, VietGPT-Wikipedia (25% chacune)
- **DurÃ©e**: ~3-4 heures

### **3. ModÃ¨le 124M - Phase A (Foundation)**
- **Fichier**: `training_configs/model_124M_phase_a.json`
- **Usage**: Phase A du curriculum learning pour 124M
- **Tokens**: 1.5B tokens (contenu structurÃ© haute qualitÃ©)
- **Sources**: VietGPT-Wikipedia(50%), Gutenberg(33%), FineWeb-Edu(17%)
- **StratÃ©gie**: Fondations linguistiques solides
- **DurÃ©e**: ~8-12 heures

### **4. ModÃ¨le 124M - Phase B (Robustness)**
- **Fichier**: `training_configs/model_124M_phase_b.json`
- **Usage**: Phase B du curriculum learning pour 124M
- **Tokens**: 1.5B tokens (contenu web diversifiÃ©)
- **Sources**: C4-WebCrawl(67%), FineWeb-Discussions(33%)
- **StratÃ©gie**: Robustesse et gÃ©nÃ©ralisation
- **DurÃ©e**: ~8-12 heures

### **5. Test Rapide**
- **Fichier**: `training_configs/demo_fast.json`
- **Usage**: Validation rapide du pipeline
- **Tokens**: 100K tokens
- **Sources**: TinyStories + WikiText-2
- **DurÃ©e**: ~10-15 minutes

---

## ğŸš€ Commandes d'Utilisation

### **ğŸ”¤ Tokenizer Optimal (Une Fois)**
```bash
# EntraÃ®ner le tokenizer rÃ©utilisable (25M tokens)
python run_modular_pipeline.py --config tokenizer --step all
```

### **ğŸ¤– ModÃ¨le Tiny 23M**
```bash
# Dataset complet 23M (500M tokens)
python run_modular_pipeline.py --config tiny_23M --step all
```

### **ğŸ“ ModÃ¨le 124M - Curriculum Learning**
```bash
# Option 1: Pipeline automatique complet (2 phases)
python run_curriculum_124M.py --step all

# Option 2: Phase par phase
python run_curriculum_124M.py --step phase_a    # Foundation (1.5B tokens)
python run_curriculum_124M.py --step phase_b    # Robustness (1.5B tokens)

# Option 3: Phases individuelles avec contrÃ´le granulaire
python run_modular_pipeline.py --config model_124M_phase_a --step all
python run_modular_pipeline.py --config model_124M_phase_b --step all
```

### **Test Rapide (Validation)**
```bash
# Pipeline complet de test
python scripts/01_prepare_corpus.py \
  --config config/modular_pipeline/training_configs/demo_fast.json \
  --output-dir data/modular/demo_corpus

python scripts/02_train_tokenizer.py \
  --manifest data/modular/demo_corpus/manifest.json \
  --output-dir data/modular/demo_tokenizer \
  --vocab-size 8192

python scripts/03_pack_sequences.py \
  --manifest data/modular/demo_corpus/manifest.json \
  --tokenizer-dir data/modular/demo_tokenizer \
  --output-dir data/modular/demo_packed \
  --sequence-length 512
```

---

## ğŸ”§ Avantages de la Nouvelle Organisation

### âœ… **SÃ©paration Claire**
- **Legacy**: `config/datasets/` et `config/meta_configs/`
- **Modular**: `config/modular_pipeline/`

### âœ… **Format JSON Uniforme**
- Fini les mÃ©langes YAML/JSON
- Structure cohÃ©rente et lisible

### âœ… **Organisation Logique**
- **Sources individuelles**: descriptions et paramÃ¨tres par dataset
- **Configs d'entraÃ®nement**: compositions complÃ¨tes pour usage spÃ©cifique

### âœ… **FlexibilitÃ© Maximale**
- RÃ©utilisation facile des sources
- ParamÃ¨tres centralisÃ©s
- TraÃ§abilitÃ© complÃ¨te

---

## ğŸ“Š RÃ©sultats Attendus

### **Tokenizer (7M tokens)**
- Vocabulaire: 32,768 tokens
- Fichiers: `spm.model`, `spm.vocab`, `tokenizer_config.json`
- Temps: 20-30 minutes

### **Dataset 500M (Tiny 23M)**
- SÃ©quences training: ~478,000 (1024 tokens chacune)
- SÃ©quences validation: ~10,000
- Fichiers: `train.bin`, `val.bin`, `final_manifest.json`
- Taille: ~2GB compressÃ©

### **Dataset Demo (100K)**
- SÃ©quences training: ~190 (512 tokens chacune)
- Vocabulaire: 8,192 tokens
- Temps total: 10-15 minutes