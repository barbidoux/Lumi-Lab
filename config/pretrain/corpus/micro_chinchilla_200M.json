{
  "name": "micro_chinchilla_200M_dataset",
  "description": "Dataset for micro model (9.4M params) - 200M tokens Chinchilla-optimal",
  "version": "1.0.0",
  "model_target": "9.4M parameters",
  "target_total_tokens": 200000000,
  "datasets_count": 2,
  "chinchilla_ratio": "21:1 tokens per parameter",
  "sequence_length": 1024,
  "vocab_size": 32768,
  "strategy": "Balanced two-source training: web diversity + encyclopedic knowledge",

  "processing_params": {
    "shard_size": 10000,
    "vocab_size": 32768
  },

  "deduplication": {
    "threshold": 0.85,
    "num_perm": 128,
    "shingle_size": 3
  },

  "training_params": {
    "train_ratio": 0.98,
    "shard_tokens": 2000000,
    "sequence_length": 1024,
    "vocab_size": 32768,
    "tokenizer_path": "data/models/tokenizers/spm_32k"
  },

  "sources": {
    "c4_micro": {
      "type": "huggingface",
      "dataset_name": "allenai/c4",
      "subset": "en",
      "split": "train",
      "text_keys": "text",
      "token_budget": 100000000,
      "weight": 0.50,
      "description": "C4 English - diverse web content for generalization",
      "analysis_sample_size": 2000,
      "chars_per_token": 4.0,
      "margin_ratio": 0.02,
      "trust_remote_code": false,
      "min_length": 50,
      "max_length": 10000,
      "require_english": true
    },

    "vietgpt_wikipedia_micro": {
      "type": "huggingface",
      "dataset_name": "vietgpt/wikipedia_en",
      "subset": null,
      "split": "train",
      "text_keys": "text",
      "token_budget": 100000000,
      "weight": 0.50,
      "description": "VietGPT Wikipedia - encyclopedic foundation with high quality",
      "analysis_sample_size": 1500,
      "chars_per_token": 4.0,
      "margin_ratio": 0.02,
      "trust_remote_code": false,
      "min_length": 100,
      "max_length": 50000,
      "require_english": true
    }
  }
}
