{
  "name": "lora_base",
  "description": "LoRA configuration for base 124M model fine-tuning - Clean v3.0",
  "version": "3.0",
  "model_target": "124M parameters (24 layers, 768 d_model, 12 heads)",

  "training_params": {
    "learning_rate": 3e-5,
    "num_train_epochs": -1,
    "max_steps": 18000,
    "per_device_train_batch_size": 2,
    "per_device_eval_batch_size": 4,
    "gradient_accumulation_steps": 16,
    "warmup_ratio": 0.1,
    "weight_decay": 0.01,
    "logging_steps": 50,
    "save_steps": 1000,
    "eval_steps": 500,
    "evaluation_strategy": "steps",
    "save_strategy": "steps",
    "load_best_model_at_end": true,
    "metric_for_best_model": "eval_loss",
    "greater_is_better": false,
    "save_total_limit": 5,
    "seed": 42,
    "data_seed": 42,
    "dataloader_num_workers": 4,
    "remove_unused_columns": false,
    "optim": "adamw_torch",
    "lr_scheduler_type": "cosine",
    "fp16": false,
    "bf16": true,
    "gradient_checkpointing": true,
    "ddp_find_unused_parameters": false,
    "group_by_length": false,
    "report_to": ["none"],
    "max_grad_norm": 1.0,
    "adam_beta1": 0.9,
    "adam_beta2": 0.95,
    "adam_epsilon": 1e-8
  },

  "lora_config": {
    "r": 64,
    "lora_alpha": 128,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_dropout": 0.05,
    "bias": "none",
    "fan_in_fan_out": false,
    "init_lora_weights": true
  },

  "dataset_config": {
    "interleave_strategy": {
      "seed": 42,
      "stopping_strategy": "all_exhausted"
    }
  },

  "evaluation_config": {
    "generation_config": {
      "max_new_tokens": 128,
      "temperature": 0.7,
      "top_p": 0.9,
      "top_k": 50,
      "do_sample": true,
      "pad_token_id": 0,
      "eos_token_id": 2,
      "repetition_penalty": 1.1,
      "no_repeat_ngram_size": 3
    },
    "eval_prompts": [
      "What is artificial intelligence and how does it work?",
      "Explain the concept of machine learning in simple terms.",
      "How do neural networks learn patterns from data?",
      "What are the main applications of AI in everyday life?",
      "Describe the difference between AI, machine learning, and deep learning."
    ]
  },

  "_performance_rationale": {
    "learning_rate": "3e-5 optimal for 124M with LoRA r=64",
    "batch_size": "32 effective (2Ã—16 grad_accum) for stability",
    "lora_rank": "r=64 with alpha=128 for optimal adaptation capacity",
    "warmup": "10% warmup_ratio for stability",
    "scheduler": "Cosine for smooth convergence",
    "gradient_checkpointing": "true for 124M model (essential for VRAM)",
    "report_to": "none (TensorBoard disabled to avoid callback errors)"
  },

  "_training_estimates": {
    "optimal_steps": "18000 steps (2 epochs through dataset)",
    "training_time": "16-20 hours on RTX 4090",
    "memory_usage": "~14-16GB VRAM with gradient checkpointing",
    "total_train_tokens": "~80M tokens (0.65 tokens/param for 124M model)",
    "dataset_size": "~350k conversations recommended"
  }
}
