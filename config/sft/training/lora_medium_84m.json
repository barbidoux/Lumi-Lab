{
  "name": "lora_optimal_medium_84m",
  "description": "Optimal SFT configuration for medium 84M model - Clean v1.0",
  "version": "1.0",
  "model_target": "84M parameters (12 layers, 512 d_model, 8 heads)",

  "training_params": {
    "learning_rate": 4.5e-5,
    "num_train_epochs": -1,
    "max_steps": 5000,
    "per_device_train_batch_size": 4,
    "per_device_eval_batch_size": 8,
    "gradient_accumulation_steps": 8,
    "warmup_ratio": 0.1,
    "weight_decay": 0.01,
    "logging_steps": 50,
    "save_steps": 500,
    "eval_steps": 100,
    "evaluation_strategy": "steps",
    "save_strategy": "steps",
    "load_best_model_at_end": true,
    "metric_for_best_model": "eval_loss",
    "greater_is_better": false,
    "save_total_limit": 5,
    "seed": 42,
    "data_seed": 42,
    "dataloader_num_workers": 4,
    "remove_unused_columns": false,
    "optim": "adamw_torch",
    "lr_scheduler_type": "cosine",
    "fp16": false,
    "bf16": true,
    "gradient_checkpointing": true,
    "ddp_find_unused_parameters": false,
    "group_by_length": false,
    "report_to": ["none"],
    "max_grad_norm": 1.0,
    "adam_beta1": 0.9,
    "adam_beta2": 0.95,
    "adam_epsilon": 1e-8
  },

  "lora_config": {
    "r": 56,
    "lora_alpha": 112,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_dropout": 0.05,
    "bias": "none",
    "fan_in_fan_out": false,
    "init_lora_weights": true
  },

  "dataset_config": {
    "interleave_strategy": {
      "seed": 42,
      "stopping_strategy": "all_exhausted"
    }
  },

  "evaluation_config": {
    "generation_config": {
      "max_new_tokens": 128,
      "temperature": 0.7,
      "top_p": 0.9,
      "top_k": 50,
      "do_sample": true,
      "pad_token_id": 0,
      "eos_token_id": 2,
      "repetition_penalty": 1.1,
      "no_repeat_ngram_size": 3
    },
    "eval_prompts": [
      "What is artificial intelligence and how does it work?",
      "Explain the concept of machine learning in simple terms.",
      "How do neural networks learn patterns from data?",
      "What are the main applications of AI in everyday life?",
      "Describe the difference between AI, machine learning, and deep learning."
    ]
  },

  "_performance_rationale": {
    "learning_rate": "4.5e-5 optimal for 84M with LoRA r=56 (interpolated from small/base)",
    "batch_size": "32 effective (4x8 grad_accum) for maximum stability",
    "lora_rank": "r=56 with alpha=112 for increased adaptation capacity (interpolated small=48, base=64)",
    "warmup": "10% warmup_ratio for critical initial stability",
    "scheduler": "Cosine for smooth convergence without overfitting",
    "weight_decay": "0.01 for light regularization (LoRA already regularized)",
    "eval_frequency": "Evaluation every 375 steps for fine-grained monitoring",
    "gradient_checkpointing": "true (84M model benefits from VRAM savings)",
    "report_to": "none (TensorBoard disabled to avoid callback errors)"
  },

  "_training_estimates": {
    "optimal_steps": "13500 steps (2 epochs through ~200k conversations)",
    "training_time": "6-8 hours on RTX 4090",
    "memory_usage": "~10-12GB VRAM with gradient checkpointing",
    "convergence_step": "~6500-7500 steps",
    "best_eval_loss_target": "<1.2",
    "expected_boolq": "72-77% accuracy post-SFT",
    "epochs_at_max_steps": "2.0 epochs (optimal for SFT without overfitting)",
    "total_train_tokens": "~55M tokens (0.65 tokens/param, BALANCED for LoRA SFT)",
    "dataset_size": "~200-250k conversations (OASST1 + WizardLM + Open-Orca)"
  }
}
