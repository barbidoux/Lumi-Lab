{
  "name": "lora_small",
  "description": "LoRA configuration for small 42M model fine-tuning - Clean v3.0",
  "version": "3.0",
  "model_target": "42M parameters (12 layers, 512 d_model, 8 heads)",

  "training_params": {
    "learning_rate": 5e-5,
    "num_train_epochs": -1,
    "max_steps": 10000,
    "per_device_train_batch_size": 4,
    "per_device_eval_batch_size": 8,
    "gradient_accumulation_steps": 8,
    "warmup_ratio": 0.1,
    "weight_decay": 0.01,
    "logging_steps": 25,
    "save_steps": 500,
    "eval_steps": 250,
    "evaluation_strategy": "steps",
    "save_strategy": "steps",
    "load_best_model_at_end": true,
    "metric_for_best_model": "eval_loss",
    "greater_is_better": false,
    "save_total_limit": 5,
    "seed": 42,
    "data_seed": 42,
    "dataloader_num_workers": 4,
    "remove_unused_columns": false,
    "optim": "adamw_torch",
    "lr_scheduler_type": "cosine",
    "fp16": false,
    "bf16": true,
    "gradient_checkpointing": true,
    "ddp_find_unused_parameters": false,
    "group_by_length": false,
    "report_to": ["none"],
    "max_grad_norm": 1.0,
    "adam_beta1": 0.9,
    "adam_beta2": 0.95,
    "adam_epsilon": 1e-8
  },

  "lora_config": {
    "r": 32,
    "lora_alpha": 64,
    "target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "lora_dropout": 0.05,
    "bias": "none",
    "fan_in_fan_out": false,
    "init_lora_weights": true
  },

  "dataset_config": {
    "interleave_strategy": {
      "seed": 42,
      "stopping_strategy": "all_exhausted"
    }
  },

  "evaluation_config": {
    "generation_config": {
      "max_new_tokens": 128,
      "temperature": 0.7,
      "top_p": 0.9,
      "top_k": 50,
      "do_sample": true,
      "pad_token_id": 0,
      "eos_token_id": 2,
      "repetition_penalty": 1.1,
      "no_repeat_ngram_size": 3
    },
    "eval_prompts": [
      "What is artificial intelligence and how does it work?",
      "Explain the concept of machine learning in simple terms.",
      "How do neural networks learn patterns from data?",
      "What are the main applications of AI in everyday life?",
      "Describe the difference between AI, machine learning, and deep learning."
    ]
  },

  "_performance_rationale": {
    "learning_rate": "5e-5 optimal for 42M with LoRA r=32",
    "batch_size": "32 effective (4Ã—8 grad_accum) for stability",
    "lora_rank": "r=32 with alpha=64 for optimal adaptation",
    "warmup": "10% warmup_ratio for stability",
    "scheduler": "Cosine for smooth convergence",
    "gradient_checkpointing": "true for 42M model (reduces VRAM ~40%)",
    "report_to": "none (TensorBoard disabled to avoid callback errors)"
  },

  "_training_estimates": {
    "optimal_steps": "10000 steps (2 epochs through dataset)",
    "training_time": "8-10 hours on RTX 4090",
    "memory_usage": "~10-12GB VRAM with gradient checkpointing",
    "total_train_tokens": "~27M tokens (0.65 tokens/param for 42M model)",
    "dataset_size": "~120k conversations recommended"
  }
}
