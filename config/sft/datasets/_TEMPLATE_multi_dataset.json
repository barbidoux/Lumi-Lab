{
  "_comment": "TEMPLATE: Multi-Dataset SFT Corpus Configuration (v3.0)",
  "_description": "This template shows how to configure multiple datasets with weighted sampling",

  "name": "multi_dataset_mix",
  "description": "Balanced mix of multiple high-quality datasets",
  "version": "3.0",
  "template": "chatml",

  "sampling_strategy": {
    "_comment": "Token budget mode recommended for multi-dataset training",
    "mode": "target_tokens",

    "target_tokens": 40000000,
    "_target_tokens_comment": "40M tokens = ~1.7 tokens/param for 23M model (balanced for LoRA SFT)",
    "_tokens_per_param_guide": {
      "23M_model": {
        "light": "20-30M tokens (0.9-1.3 tokens/param)",
        "balanced": "40-50M tokens (1.7-2.2 tokens/param)",
        "optimal": "60-80M tokens (2.6-3.5 tokens/param)"
      },
      "42M_model": {
        "light": "40-50M tokens",
        "balanced": "70-90M tokens",
        "optimal": "100-130M tokens"
      },
      "124M_model": {
        "light": "120-150M tokens",
        "balanced": "210-270M tokens",
        "optimal": "300-400M tokens"
      }
    },

    "seed": 42,
    "_seed_comment": "Random seed for reproducible dataset sampling",
    "chars_per_token": 4.0,
    "_chars_per_token_comment": "Estimation for token counting (4 chars/token average)",
    "analysis_sample_size": 100,
    "_analysis_sample_size_comment": "Number of samples to analyze for token estimation"
  },

  "output_params": {
    "sequence_length": 1024,
    "_sequence_length_comment": "Maximum sequence length (should match model training config)",
    "shard_size": 2000,
    "_shard_size_comment": "Number of conversations per shard file",
    "train_ratio": 0.95,
    "_train_ratio_comment": "Train/validation split ratio (0.95 = 95% train, 5% validation)",
    "enable_packing": false,
    "_packing_note": "Packing=false recommended for conversational quality. Set to true for throughput (must match training config)",
    "batch_size": 1000,
    "_batch_size_comment": "Number of conversations to batch before flushing to disk (memory vs I/O trade-off)"
  },

  "quality_filters": {
    "min_prompt_length": 10,
    "_min_prompt_length_comment": "Minimum prompt length in characters",
    "max_prompt_length": 1024,
    "_max_prompt_length_comment": "Maximum prompt length in characters",
    "min_response_length": 20,
    "_min_response_length_comment": "Minimum response length in characters",
    "max_response_length": 2048,
    "_max_response_length_comment": "Maximum response length in characters",
    "filter_urls": false,
    "_filter_urls_comment": "Whether to filter out conversations with URLs",
    "filter_code_blocks": false,
    "_filter_code_blocks_comment": "Whether to filter out conversations with code blocks",
    "require_ascii": false,
    "_require_ascii_comment": "Whether to require ASCII-only text",
    "remove_duplicates": true,
    "_remove_duplicates_comment": "Whether to remove duplicate conversations",
    "deduplication": {
      "threshold": 0.85,
      "_threshold_comment": "MinHash LSH similarity threshold (0.85 = 85% similar)",
      "num_perm": 128,
      "_num_perm_comment": "Number of permutations for MinHash (higher = more accurate but slower)"
    }
  },

  "datasets": [
    {
      "name": "high_quality_dataset_1",
      "type": "huggingface",
      "dataset_name": "databricks/databricks-dolly-15k",
      "_dataset_name_comment": "HuggingFace Hub dataset identifier",
      "subset": null,
      "_subset_comment": "Dataset subset/config (null = default)",
      "split": "train",
      "_split_comment": "Dataset split to use (train/validation/test)",
      "text_fields": {
        "prompt": "instruction",
        "response": "response",
        "context": "context"
      },
      "_text_fields_comment": "Mapping from standard fields (prompt/response) to dataset-specific column names",
      "weight": 1.5,
      "_weight_comment": "Relative weight for sampling (1.5 = 50% more samples than weight 1.0). In target_tokens mode, this controls token budget allocation.",
      "description": "Premium quality human-generated instructions"
    },
    {
      "name": "high_quality_dataset_2",
      "type": "huggingface",
      "dataset_name": "OpenAssistant/oasst1",
      "subset": null,
      "split": "train",
      "text_fields": {
        "prompt": "text",
        "response": "text"
      },
      "weight": 1.5,
      "_weight_comment": "Equal weight to dataset_1 (same priority)",
      "description": "Premium quality conversational data",
      "custom_processing": {
        "conversation_tree": true,
        "_conversation_tree_comment": "Extract conversations from tree structure",
        "filter_language": "en",
        "_filter_language_comment": "Only keep English conversations",
        "min_score": 2,
        "_min_score_comment": "Minimum quality score for messages",
        "role_filter": ["prompter", "assistant"],
        "_role_filter_comment": "Only keep messages from these roles"
      }
    },
    {
      "name": "baseline_dataset",
      "type": "huggingface",
      "dataset_name": "tatsu-lab/alpaca",
      "subset": null,
      "split": "train",
      "text_fields": {
        "prompt": "instruction",
        "input": "input",
        "response": "output"
      },
      "weight": 0.3,
      "_weight_comment": "Lower weight (0.3) = reduced sampling. In target_tokens mode: 0.3/(1.5+1.5+0.3) = 9% of total tokens",
      "description": "Baseline instruction-following dataset"
    }
  ],

  "processing_metadata": {
    "processing_time_estimate": "45-60 minutes",
    "quality_focus": "premium_datasets_with_balanced_token_budget",
    "template_rationale": "ChatML format for conversational quality",
    "dataset_ratio": "5:5:1 (weights 1.5:1.5:0.3) prioritizes premium quality",
    "training_efficiency": "~10,800 steps for 2 epochs, 5-6 hours on RTX 4090"
  }
}
