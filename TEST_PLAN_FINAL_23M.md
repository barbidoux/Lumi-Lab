# Plan de Test Complet - Mod√®le 23M (Architecture R√©elle)
## Pipeline de A √† Z: Zero ‚Üí √âvaluation SFT

**Date**: 2025-10-02
**Objectif**: Tester le pipeline complet avec l'architecture r√©elle refactoris√©e
**Mod√®le**: Tiny 23M - 600M tokens Chinchilla-optimal
**Dur√©e estim√©e**: ~6-10 heures

---

## üìã Architecture du Pipeline

### Datasets Utilis√©s

#### Pour le Tokenizer (100M tokens)
- **Config**: `config/pretrain/corpus/tokenizer_training_mix.json`
- **Sources**:
  - C4: 28M tokens (28%)
  - Gutenberg: 24M tokens (24%)
  - FineWeb-Edu: 24M tokens (24%)
  - Wikipedia: 24M tokens (24%)

#### Pour le Pr√©-training (600M tokens)
- **Config**: `config/pretrain/corpus/tiny_23M_chinchilla_500M.json`
- **Sources**:
  - C4: 150M tokens (25%)
  - Gutenberg: 150M tokens (25%)
  - FineWeb-Edu: 150M tokens (25%)
  - Wikipedia: 150M tokens (25%)

---

## üóÇÔ∏è Structure Attendue

```
data/
‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer_corpus/          # √âtape 1 (100M tokens)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cache/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ shards/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ shard_0000.jsonl.gz
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (50 shards)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ manifest.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ plan.json
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ tiny_23M_corpus/            # √âtape 3 (600M tokens)
‚îÇ       ‚îú‚îÄ‚îÄ cache/
‚îÇ       ‚îú‚îÄ‚îÄ shards/
‚îÇ       ‚îú‚îÄ‚îÄ manifest.json
‚îÇ       ‚îî‚îÄ‚îÄ plan.json
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ tokenizers/
‚îÇ       ‚îî‚îÄ‚îÄ spm_32k/                # √âtape 2
‚îÇ           ‚îú‚îÄ‚îÄ spm.model
‚îÇ           ‚îú‚îÄ‚îÄ spm.vocab
‚îÇ           ‚îú‚îÄ‚îÄ tokenizer_config.json
‚îÇ           ‚îî‚îÄ‚îÄ TOKENIZER_CARD.md
‚îÇ
‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îî‚îÄ‚îÄ tiny_23M_1024/              # √âtape 4
‚îÇ       ‚îú‚îÄ‚îÄ train.bin
‚îÇ       ‚îú‚îÄ‚îÄ train.idx
‚îÇ       ‚îú‚îÄ‚îÄ val.bin
‚îÇ       ‚îú‚îÄ‚îÄ val.idx
‚îÇ       ‚îî‚îÄ‚îÄ manifest.json
‚îÇ
‚îî‚îÄ‚îÄ sft_processed/                  # √âtape 7
    ‚îú‚îÄ‚îÄ alpaca_chatml/
    ‚îî‚îÄ‚îÄ oasst1_chatml/

checkpoints/
‚îú‚îÄ‚îÄ pretrain/
‚îÇ   ‚îî‚îÄ‚îÄ tiny_23M/                   # √âtape 5
‚îÇ       ‚îú‚îÄ‚îÄ step_5000/
‚îÇ       ‚îî‚îÄ‚îÄ final/
‚îî‚îÄ‚îÄ sft/
    ‚îî‚îÄ‚îÄ tiny_23M/                   # √âtape 8
        ‚îî‚îÄ‚îÄ final/

evaluation_results/
‚îú‚îÄ‚îÄ pretrain/tiny_23M/              # √âtape 6
‚îî‚îÄ‚îÄ sft/tiny_23M/                   # √âtape 9
```

---

## üìù Plan de Test D√©taill√©

### √âTAPE 1: Pr√©paration Corpus Tokenizer (100M tokens)

#### 1.1 Analyse du Plan (Optionnel - V√©rification)
```bash
python scripts/10_prepare_corpus.py \
    --config config/pretrain/corpus/tokenizer_training_mix.json \
    --output-dir data/datasets/tokenizer_corpus \
    --analyze-only \
    --log-level INFO
```

**Note**: `shard_size` est maintenant dans le config (`processing_params.shard_size`, d√©faut=50000), pas en CLI.

**V√©rifications**:
- [ ] Plan g√©n√©r√©: `data/datasets/tokenizer_corpus/plan.json`
- [ ] Rapport affich√©:
  - C4: ~34K samples (28M tokens)
  - Gutenberg: ~118 samples (24M tokens)
  - FineWeb: ~13K samples (24M tokens)
  - Wikipedia: ~2.3K samples (24M tokens)
  - **TOTAL: ~50K samples, 100M tokens**

---

#### 1.2 G√©n√©ration du Corpus Tokenizer
```bash
python scripts/10_prepare_corpus.py \
    --config config/pretrain/corpus/tokenizer_training_mix.json \
    --output-dir data/datasets/tokenizer_corpus \
    --use-cache \
    --log-level INFO \
    2>&1 | tee logs/01_tokenizer_corpus.log
```

**V√©rifications en Cours**:
- [ ] **Phase 1 - Streaming Processing**:
  - 4 sources trait√©es s√©quentiellement
  - Chaque source: `üöÄStreaming {source}: XXXtokens`
  - Memory: CONSTANT (~50MB max)
  - Token budget atteint pour chaque source

- [ ] **Phase 2 - Assembly**:
  - `üîß Starting Phase 2: TRUE STREAMING ASSEMBLY`
  - 50 shards cr√©√©s
  - Deduplication globale: ~0.3%
  - Target tokens: 100M

**Sortie Attendue**:
```
‚úÖ TRUE STREAMING processing complete!
   üìä Final documents: ~83K
   üéØ Exact tokens: ~100M
   üíæ Peak memory: <100MB
   üì¶ Output shards: 50
```

**Fichiers Cr√©√©s**:
- [ ] `data/datasets/tokenizer_corpus/manifest.json`
- [ ] `data/datasets/tokenizer_corpus/shards/` (50 fichiers .jsonl.gz)
- [ ] `data/datasets/tokenizer_corpus/cache/` (fichiers temporaires)

**Dur√©e estim√©e**: 10-15 min

---

### √âTAPE 2: Training Tokenizer (32K vocab)

```bash
python scripts/20_train_tokenizer.py \
    --config config/pretrain/tokenizer/spm32k.json \
    --output-dir data/models/tokenizers/spm_32k \
    --log-level INFO \
    2>&1 | tee logs/02_tokenizer_training.log
```

**V√©rifications en Cours**:
- [ ] **Integrity Verification**: Tous les shards v√©rifi√©s
- [ ] **Corpus Analysis**:
  - 50/50 shards trait√©s
  - Stats: Docs, Sentences, Tokens affich√©s
- [ ] **Sentence Streaming**:
  - `üìä Progress bar` avec sentences/s
  - Target: 100M sentences max
  - Actual: ~2.8M sentences written
- [ ] **SentencePiece Training**:
  - EM training iterations
  - Shrinking vocabulary
  - Final vocab: 32,768

**Sortie Attendue**:
```
‚úÖ Tokenizer training complete!
üìÅ Output: data/models/tokenizers/spm_32k/
   üìù spm.model (vocab_size=32768)
   üìä Compression ratio: ~4.1 chars/token
   üéØ Fertility: 1.15
   üìà Coverage: 99.95%
```

**Fichiers Cr√©√©s**:
- [ ] `spm.model` (le tokenizer)
- [ ] `spm.vocab` (vocabulaire)
- [ ] `tokenizer_config.json` (config + SHA256)
- [ ] `TOKENIZER_CARD.md` (m√©triques)

**SHA256 CRITIQUE**:
```bash
# Noter le hash pour validation
cat data/models/tokenizers/spm_32k/tokenizer_config.json | jq '.sha256_hash'
# Exemple: "a1b2c3d4..."
```

**Dur√©e estim√©e**: 5-10 min

---

### √âTAPE 3: Pr√©paration Corpus Pr√©-training (600M tokens)

#### 3.1 Analyse du Plan
```bash
python scripts/10_prepare_corpus.py \
    --config config/pretrain/corpus/tiny_23M_chinchilla_500M.json \
    --output-dir data/datasets/tiny_23M_corpus \
    --analyze-only \
    --log-level INFO
```

**V√©rifications**:
- [ ] Plan avec 4 sources:
  - C4: 150M tokens (25%)
  - Gutenberg: 150M tokens (25%)
  - FineWeb: 150M tokens (25%)
  - Wikipedia: 150M tokens (25%)
  - **TOTAL: 600M tokens**

---

#### 3.2 G√©n√©ration du Corpus Complet
```bash
python scripts/10_prepare_corpus.py \
    --config config/pretrain/corpus/tiny_23M_chinchilla_500M.json \
    --output-dir data/datasets/tiny_23M_corpus \
    --use-cache \
    --log-level INFO \
    2>&1 | tee logs/03_pretrain_corpus.log
```

**V√©rifications**:
- [ ] 4 sources trait√©es (C4, Gutenberg, FineWeb, Wikipedia)
- [ ] Chaque source: 150M tokens
- [ ] Assembly: ~300 shards (600M tokens / 2M per shard)
- [ ] Deduplication globale appliqu√©e

**Sortie Attendue**:
```
‚úÖ TRUE STREAMING processing complete!
   üìä Final documents: ~500K
   üéØ Exact tokens: ~600M
   üíæ Peak memory: <100MB
   üì¶ Output shards: ~300
```

**Dur√©e estim√©e**: 30-60 min (d√©pend du streaming HuggingFace)

---

### √âTAPE 4: Packing du Dataset (Tokenization + S√©quences)

```bash
python scripts/30_pack_dataset.py \
    --config config/pretrain/packing/default.json \
    --corpus-dir data/datasets/tiny_23M_corpus \
    --tokenizer-dir data/models/tokenizers/spm_32k \
    --output-dir data/processed/tiny_23M_1024 \
    --log-level INFO \
    2>&1 | tee logs/04_packing.log
```

**V√©rifications**:
- [ ] Tokenizer charg√©: SHA256 valid√©
- [ ] Corpus manifest charg√©
- [ ] Tokenization streaming (shards trait√©s un par un)
- [ ] Packing: s√©quences de 1024 tokens
- [ ] Train/val split: 98/2

**Sortie Attendue**:
```
‚úÖ Sequence packing complete!
   üìä Total tokens: ~600M
   üéØ Sequences: ~586K (1024 tokens each)
   üìÇ Train: ~574K sequences (98%)
   üìÇ Val: ~12K sequences (2%)
   üíæ Files:
      - train.bin (~1.1 GB)
      - train.idx
      - val.bin (~23 MB)
      - val.idx
```

**Fichiers Cr√©√©s**:
- [ ] `train.bin` + `train.idx`
- [ ] `val.bin` + `val.idx`
- [ ] `manifest.json` (avec tokenizer_sha256)

**Validation Tokenizer**:
```bash
cat data/processed/tiny_23M_1024/manifest.json | jq '.tokenizer_sha256'
# DOIT √™tre identique au tokenizer
```

**Dur√©e estim√©e**: 20-40 min

---

### √âTAPE 5: Pr√©-training (23M param√®tres)

```bash
accelerate launch --mixed_precision bf16 scripts/40_pretrain.py \
    --config config/pretrain/training/chinchilla_tiny_500m.json \
    --data_dirs data/processed/tiny_23M_1024 \
    --tokenizer_dir data/models/tokenizers/spm_32k \
    --output_dir checkpoints/pretrain \
    --num_workers 4 \
    2>&1 | tee logs/05_pretrain.log
```

**Config Utilis√©e** (`chinchilla_tiny_500m.json`):
- Architecture: `config/architectures/tiny.json` (23M params)
- Max steps: 30,000
- Batch size: 8 per device
- Gradient accumulation: 4
- Learning rate: 3e-4 (cosine + warmup 3K steps)
- Eval steps: 2,500
- Save steps: 5,000

**V√©rifications en Cours**:
- [ ] **D√©marrage**:
  - Tokenizer SHA256 valid√©
  - Model: "23M parameters"
  - FlashAttention-2 activ√© (ou SDPA fallback)
  - Dataset: 574K train sequences

- [ ] **Training**:
  - Loss diminue: ~10 ‚Üí <3.0
  - Learning rate: warmup puis cosine decay
  - GPU util > 85%
  - VRAM: ~8-12 GB

- [ ] **Checkpoints**:
  - Tous les 5,000 steps
  - Contient: model, optimizer, scheduler, meta, rng_state

- [ ] **√âvaluation** (tous les 2,500 steps):
  - Validation perplexity calcul√©e
  - Devrait diminuer au fil du training

**Sortie Attendue**:
```
‚úÖ Training complete!
   üìä Steps: 30,000
   üìâ Final loss: 2.8-3.2
   ‚è±Ô∏è  Duration: 2-4h
   üíæ Checkpoints: step_5000, 10000, ..., 30000, final
```

**Dur√©e estim√©e**: 2-4 heures (RTX 4090)

---

### √âTAPE 6: √âvaluation Pr√©-training

```bash
python scripts/50_evaluate_pretrain.py \
    --model_path checkpoints/pretrain/tiny/final \
    --tokenizer_path data/models/tokenizers/spm_32k \
    --output_dir evaluation_results/pretrain/tiny \
    --log-level INFO \
    2>&1 | tee logs/06_eval_pretrain.log
```

**M√©triques Attendues**:
- [ ] **WikiText-2 Perplexity**: 45-70
- [ ] **BoolQ Accuracy**: 58-68%
- [ ] **G√©n√©rations**: Coh√©rentes mais pas conversationnelles

**Dur√©e estim√©e**: 10-15 min

---

### √âTAPE 7: Pr√©paration Corpus SFT

#### 7.1 Alpaca (ChatML)
```bash
python scripts/60_prepare_sft_corpus.py \
    --config config/sft/datasets/alpaca_chatml.json \
    --tokenizer_dir data/models/tokenizers/spm_32k \
    --output_dir data/sft_processed/alpaca_chatml \
    --log-level INFO \
    2>&1 | tee logs/07a_sft_alpaca.log
```

#### 7.2 OASST1 (ChatML)
```bash
python scripts/60_prepare_sft_corpus.py \
    --config config/sft/datasets/oasst1_chatml.json \
    --tokenizer_dir data/models/tokenizers/spm_32k \
    --output_dir data/sft_processed/oasst1_chatml \
    --log-level INFO \
    2>&1 | tee logs/07b_sft_oasst1.log
```

**V√©rifications**:
- [ ] Template ChatML appliqu√©:
  ```
  <|im_start|>user
  {instruction}
  <|im_end|>
  <|im_start|>assistant
  {response}
  <|im_end|>
  ```
- [ ] Tokenizer SHA256 v√©rifi√© (identique au pretrain)
- [ ] Train/val split cr√©√©
- [ ] Final manifest avec tokenizer_config_hash

**Dur√©e estim√©e**: 5-10 min chacun

---

### √âTAPE 8: SFT Training (LoRA)

```bash
accelerate launch --mixed_precision bf16 scripts/70_train_sft.py \
    --config config/sft/training/lora_optimal_tiny_23m.json \
    --model_path checkpoints/pretrain/tiny/final \
    --data_dirs data/sft_processed/alpaca_chatml data/sft_processed/oasst1_chatml \
    --data_weights 0.7 0.3 \
    --tokenizer_path data/models/tokenizers/spm_32k \
    --output_dir checkpoints/sft/tiny \
    2>&1 | tee logs/08_sft.log
```

**Config** (`lora_optimal_tiny_23m.json`):
- LoRA: r=32, alpha=64, dropout=0.05
- Learning rate: 8e-5
- Max steps: 2,000
- Batch size: 4, grad accum: 8

**V√©rifications**:
- [ ] Base model charg√©
- [ ] LoRA adapters: r=32, alpha=64
- [ ] Dataset mix: 70% Alpaca, 30% OASST1
- [ ] Loss SFT diminue: ~2.5 ‚Üí <1.5
- [ ] VRAM < 8 GB

**Dur√©e estim√©e**: 1-2 heures

---

### √âTAPE 9: √âvaluation SFT

```bash
python scripts/80_evaluate_sft.py \
    --config config/evaluation/sft_standard.json \
    --model_path checkpoints/sft/tiny/final \
    --tokenizer_path data/models/tokenizers/spm_32k \
    --output_file evaluation_results/sft/tiny/results.json \
    2>&1 | tee logs/09_eval_sft.log
```

**M√©triques Attendues**:
- [ ] **SFT Perplexity**: 35-50
- [ ] **BoolQ Accuracy**: 68-75% (+10% vs pretrain)
- [ ] **Smoke Tests Quality**: 0.65-0.80
- [ ] **G√©n√©rations**: Coh√©rentes, structur√©es (ChatML)

**Dur√©e estim√©e**: 15-20 min

---

## üîç Validation Finale

### Tokenizer Consistency Check
```bash
echo "=== TOKENIZER SHA256 VALIDATION ==="
echo ""
echo "SOURCE:"
cat data/models/tokenizers/spm_32k/tokenizer_config.json | jq -r '.sha256_hash'
echo ""
echo "PRETRAIN DATASET:"
cat data/processed/tiny_23M_1024/manifest.json | jq -r '.tokenizer_sha256'
echo ""
echo "SFT DATASETS:"
cat data/sft_processed/alpaca_chatml/manifest.json | jq -r '.tokenizer_sha256'
cat data/sft_processed/oasst1_chatml/manifest.json | jq -r '.tokenizer_sha256'
echo ""
echo "‚ö†Ô∏è ALL MUST BE IDENTICAL!"
```

---

## üìä M√©triques de R√©f√©rence

### Pr√©-training (23M - 600M tokens)
| M√©trique | Attendu | Obtenu |
|----------|---------|--------|
| Final Loss | 2.8-3.2 | _____ |
| WikiText-2 PPL | 45-70 | _____ |
| BoolQ Accuracy | 58-68% | _____ |
| Tokens/sec (4090) | ~600-800 | _____ |
| VRAM (BF16) | ~8-12 GB | _____ |

### SFT (23M - LoRA r=32)
| M√©trique | Attendu | Obtenu |
|----------|---------|--------|
| Final Loss | 1.5-2.0 | _____ |
| SFT Perplexity | 35-50 | _____ |
| BoolQ Accuracy | 68-75% | _____ |
| Smoke Quality | 0.65-0.80 | _____ |
| VRAM (LoRA) | ~6-8 GB | _____ |

---

## ‚úÖ Crit√®res de Succ√®s

1. **Pipeline complet** sans erreur ‚úì
2. **Tokenizer SHA256** identique partout ‚úì
3. **Corpus streaming** avec <100MB memory ‚úì
4. **Pretrain loss** < 3.0 ‚úì
5. **SFT loss** < 2.0 ‚úì
6. **BoolQ am√©lioration** +10% apr√®s SFT ‚úì
7. **G√©n√©rations SFT** coh√©rentes (ChatML) ‚úì
8. **Configs utilis√©s** (pas de hardcoded) ‚úì

---

## üöÄ Commandes Rapides (TL;DR)

```bash
# SETUP
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
mkdir -p logs

# √âtape 1: Corpus Tokenizer (100M)
python scripts/10_prepare_corpus.py \
    --config config/pretrain/corpus/tokenizer_training_mix.json \
    --output-dir data/datasets/tokenizer_corpus \
    --use-cache \
    --log-level INFO

# √âtape 2: Tokenizer Training (32K)
python scripts/20_train_tokenizer.py \
    --config config/pretrain/tokenizer/spm32k.json \
    --output-dir data/models/tokenizers/spm_32k \
    --log-level INFO

# √âtape 3: Corpus Pretrain (600M)
python scripts/10_prepare_corpus.py \
    --config config/pretrain/corpus/tiny_23M_chinchilla_500M.json \
    --output-dir data/datasets/tiny_23M_corpus \
    --use-cache \
    --log-level INFO

# √âtape 4: Packing
python scripts/30_pack_dataset.py \
    --config config/pretrain/packing/default.json \
    --corpus-dir data/datasets/tiny_23M_corpus \
    --tokenizer-dir data/models/tokenizers/spm_32k \
    --output-dir data/processed/tiny_23M_1024 \
    --log-level INFO

# √âtape 5: Pretrain (avec accelerate)
accelerate launch --mixed_precision bf16 scripts/40_pretrain.py \
    --config config/pretrain/training/chinchilla_tiny_500m.json \
    --data_dirs data/processed/tiny_23M_1024 \
    --tokenizer_dir data/models/tokenizers/spm_32k \
    --output_dir checkpoints/pretrain \
    --num_workers 4

# √âtape 6: Eval Pretrain
python scripts/50_evaluate_pretrain.py \
    --model_path checkpoints/pretrain/tiny/final \
    --tokenizer_path data/models/tokenizers/spm_32k \
    --output_dir evaluation_results/pretrain/tiny

# √âtape 7: SFT Corpus
python scripts/60_prepare_sft_corpus.py \
    --config config/sft/datasets/alpaca_chatml.json \
    --tokenizer_dir data/models/tokenizers/spm_32k \
    --output_dir data/sft_processed/alpaca_chatml

python scripts/60_prepare_sft_corpus.py \
    --config config/sft/datasets/oasst1_chatml.json \
    --tokenizer_dir data/models/tokenizers/spm_32k \
    --output_dir data/sft_processed/oasst1_chatml

# √âtape 8: SFT Train (avec accelerate)
accelerate launch --mixed_precision bf16 scripts/70_train_sft.py \
    --config config/sft/training/lora_optimal_tiny_23m.json \
    --model_path checkpoints/pretrain/tiny/final \
    --data_dirs data/sft_processed/alpaca_chatml data/sft_processed/oasst1_chatml \
    --data_weights 0.7 0.3 \
    --tokenizer_path data/models/tokenizers/spm_32k \
    --output_dir checkpoints/sft/tiny

# √âtape 9: Eval SFT
python scripts/80_evaluate_sft.py \
    --config config/evaluation/sft_standard.json \
    --model_path checkpoints/sft/tiny/final \
    --tokenizer_path data/models/tokenizers/spm_32k \
    --output_file evaluation_results/sft/tiny/results.json
```

---

**Pr√™t pour le test complet! üöÄ**
