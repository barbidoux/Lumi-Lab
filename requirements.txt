torch>=2.3.0
transformers>=4.40.0
datasets>=2.18.0
accelerate>=0.30.0
trl>=0.9.0
peft>=0.11.0
bitsandbytes>=0.43.0
sentencepiece>=0.2.0
evaluate>=0.4.0
scikit-learn>=1.4.0
numpy>=1.24.0
tqdm>=4.65.0
wandb>=0.16.0
fastapi>=0.104.0
uvicorn>=0.24.0
pydantic>=2.5.0
# New dependencies for robust dataset preparation
ftfy>=6.1.0
langdetect>=1.0.9
datasketch>=1.6.0
pyyaml>=6.0.0
# Enhanced tokenizer training dependencies
nltk>=3.8.0
#flash-attn>=2.5.8  # Optional: Uncomment for FlashAttention-2 (requires CUDA), if used, please install serapately after having installed requirements above