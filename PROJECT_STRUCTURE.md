# Project Structure - Lumi-Lab v2.0

**Clean, logical, hierarchical organization**

---

## ğŸ“‚ Complete Directory Tree

```
Lumi-Lab/
â”‚
â”œâ”€â”€ ğŸ“œ README.md                    # Main documentation
â”œâ”€â”€ ğŸ“œ CLAUDE.md                    # Claude Code instructions
â”œâ”€â”€ ğŸ“œ MIGRATION.md                 # Migration guide v1â†’v2
â”œâ”€â”€ ğŸ“œ BREAKING_CHANGES.md          # Breaking changes list
â”œâ”€â”€ ğŸ“œ CLEANUP_SUMMARY.md           # Cleanup summary
â”œâ”€â”€ ğŸ“œ TODO_REMAINING.md            # Remaining tasks
â”œâ”€â”€ ğŸ“œ PROJECT_STRUCTURE.md         # This file
â”œâ”€â”€ ğŸ“œ Makefile                     # Build automation (âš ï¸ needs update)
â”œâ”€â”€ ğŸ“œ requirements.txt             # Python dependencies
â”‚
â”œâ”€â”€ ğŸ“ scripts/                     # Main pipeline scripts
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ”Ÿ PRE-TRAINING PIPELINE (10-50)
â”‚   â”œâ”€â”€ ğŸ“„ 10_prepare_corpus.py              # Clean raw data â†’ corpus
â”‚   â”œâ”€â”€ ğŸ“„ 20_train_tokenizer.py             # Train SentencePiece tokenizer
â”‚   â”œâ”€â”€ ğŸ“„ 30_pack_dataset.py                # Tokenize + pack sequences
â”‚   â”œâ”€â”€ ğŸ“„ 40_pretrain.py                    # Pre-train transformer
â”‚   â”œâ”€â”€ ğŸ“„ 50_evaluate_pretrain.py           # Evaluate pre-trained model
â”‚   â”‚
â”‚   â”œâ”€â”€ 6ï¸âƒ£0ï¸âƒ£ SFT PIPELINE (60-80)
â”‚   â”œâ”€â”€ ğŸ“„ 60_prepare_sft_corpus.py          # Prepare SFT corpus
â”‚   â”œâ”€â”€ ğŸ“„ 70_train_sft.py                   # Train with LoRA (industrial)
â”‚   â”œâ”€â”€ ğŸ“„ 80_evaluate_sft.py                # Evaluate SFT model
â”‚   â”‚
â”‚   â”œâ”€â”€ 9ï¸âƒ£0ï¸âƒ£ DPO PIPELINE (90)
â”‚   â”œâ”€â”€ ğŸ“„ 90_train_dpo.py                   # DPO training
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ’¯ SERVING (100)
â”‚   â”œâ”€â”€ ğŸ“„ 100_serve.py                      # Interactive/API serving
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ helpers/                          # Helper scripts
â”‚       â””â”€â”€ ğŸ“„ merge_lora.py                 # Merge LoRA + base model
â”‚
â”œâ”€â”€ ğŸ“ config/                      # Configuration files
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ architectures/           # Model architectures
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ tiny.json                     # 23M params (6 layers, 256 dim)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ small.json                    # 124M params (12 layers, 512 dim)
â”‚   â”‚   â””â”€â”€ ğŸ“„ base.json                     # 350M params (24 layers, 768 dim)
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ pretrain/                # Pre-training configs
â”‚   â”‚   â”œâ”€â”€ ğŸ“ sources/                      # Raw corpus sources
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ c4_english.json
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ wikipedia_en.json
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ openwebtext.json
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ gutenberg.json
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ fineweb_edu.json
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ ğŸ“ training/                     # Training configs
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ tokenizer_training_mix.json
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ chinchilla_tiny_500m.json
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ chinchilla_small_2b.json
â”‚   â”‚       â””â”€â”€ ğŸ“„ chinchilla_base_7b.json
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ sft/                     # SFT configs
â”‚   â”‚   â”œâ”€â”€ ğŸ“ datasets/                     # SFT dataset configs
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ alpaca_dolly_oasst_instruct.json  âœ… USE THIS
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ alpaca.json
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dolly15k.json
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ oasst1.json
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ README.md
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ ğŸ“ training/                     # LoRA training configs
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ lora_tiny_23m.json        # Optimal for 23M
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ lora_tiny.json            # Standard tiny
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ lora_small.json
â”‚   â”‚       â””â”€â”€ ğŸ“„ lora_base.json
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ dpo/                     # DPO configs (future)
â”‚   â”‚   â”œâ”€â”€ ğŸ“ datasets/
â”‚   â”‚   â””â”€â”€ ğŸ“ training/
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ evaluation/              # Evaluation configs
â”‚   â”‚   â””â”€â”€ ğŸ“„ smoke_test_prompts.json
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“„ tokens.json              # HuggingFace auth token (.gitignored)
â”‚
â”œâ”€â”€ ğŸ“ utils/                       # Utility modules
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ âœ… ESSENTIAL UTILS
â”‚   â”œâ”€â”€ ğŸ“„ model_utils.py                    # Model loading, checkpointing
â”‚   â”œâ”€â”€ ğŸ“„ dataset_utils.py                  # StreamingSFTDataset, loaders
â”‚   â”œâ”€â”€ ğŸ“„ sft_templates.py                  # Instruct/ChatML templates
â”‚   â”œâ”€â”€ ğŸ“„ sft_evaluation.py                 # SFT evaluation metrics
â”‚   â”œâ”€â”€ ğŸ“„ tokenizer_utils.py                # Tokenizer helpers + validate_sp32k_tokenizer()
â”‚   â”œâ”€â”€ ğŸ“„ auth.py                           # HuggingFace authentication
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ debug/                            # Debug utilities (optional)
â”‚       â”œâ”€â”€ ğŸ“„ adaptive_estimation.py
â”‚       â”œâ”€â”€ ğŸ“„ corpus_cache.py
â”‚       â”œâ”€â”€ ğŸ“„ tokenizer_validation.py
â”‚       â”œâ”€â”€ ğŸ“„ tokenizer_metrics.py
â”‚       â”œâ”€â”€ ğŸ“„ precise_token_counter.py
â”‚       â”œâ”€â”€ ğŸ“„ robust_imports.py
â”‚       â””â”€â”€ ğŸ“„ validate_architecture.py
â”‚
â”œâ”€â”€ ğŸ“ data/                        # Data directory (created during pipeline)
â”‚   â”œâ”€â”€ ğŸ“ raw/                              # Raw downloaded data
â”‚   â”œâ”€â”€ ğŸ“ corpus/                           # Stage 1: Cleaned corpus
â”‚   â”œâ”€â”€ ğŸ“ tokenizer/                        # Trained tokenizers
â”‚   â”œâ”€â”€ ğŸ“ processed/                        # Stage 3: Packed datasets
â”‚   â”œâ”€â”€ ğŸ“ sft_processed/                    # SFT processed data
â”‚   â””â”€â”€ ğŸ“ models/                           # Downloaded models
â”‚
â”œâ”€â”€ ğŸ“ checkpoints/                 # Training checkpoints
â”‚   â”œâ”€â”€ ğŸ“ pretrain/                         # Pre-training checkpoints
â”‚   â”‚   â”œâ”€â”€ ğŸ“ tiny/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ small/
â”‚   â”‚   â””â”€â”€ ğŸ“ base/
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ sft/                              # SFT checkpoints
â”‚   â”‚   â””â”€â”€ ğŸ“ {experiment_name}/
â”‚   â”‚       â”œâ”€â”€ ğŸ“ final/                    # LoRA adapters
â”‚   â”‚       â””â”€â”€ ğŸ“ merged/                   # Merged model (if --merge_adapters)
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ dpo/                              # DPO checkpoints
â”‚
â”œâ”€â”€ ğŸ“ evaluation_results/          # Evaluation outputs
â”‚   â””â”€â”€ ğŸ“„ *.json                            # Evaluation metrics
â”‚
â””â”€â”€ ğŸ“ venv/                        # Python virtual environment (.gitignored)
```

---

## ğŸ”¢ Numbering Scheme

Scripts are numbered by **tens (10, 20, 30...)** for easy insertion of new stages:

| Range | Purpose | Current Scripts |
|-------|---------|-----------------|
| **10-50** | Pre-training pipeline | 5 scripts |
| **60-80** | SFT pipeline | 3 scripts |
| **90** | DPO pipeline | 1 script |
| **100+** | Serving/inference | 1 script |

**Benefits:**
- âœ… Clear ordering
- âœ… Room for expansion (e.g., add `35_validate_dataset.py` between 30 and 40)
- âœ… Easy to memorize
- âœ… Alphabetical = execution order

---

## ğŸ“Š Config Hierarchy

```
config/
â”œâ”€â”€ architectures/      # WHAT to train (model size/architecture)
â”œâ”€â”€ pretrain/          # HOW to pre-train
â”‚   â”œâ”€â”€ sources/       #   - WHERE to get data
â”‚   â””â”€â”€ training/      #   - HOW MUCH data (Chinchilla budgets)
â”œâ”€â”€ sft/               # HOW to fine-tune
â”‚   â”œâ”€â”€ datasets/      #   - WHICH SFT datasets
â”‚   â””â”€â”€ training/      #   - WHICH LoRA config
â”œâ”€â”€ dpo/               # HOW to align (future)
â””â”€â”€ evaluation/        # HOW to evaluate
```

**Design Principles:**
1. **Separation of concerns** - Architecture vs data vs training
2. **Hierarchical** - Top-level = pipeline stage
3. **Self-documenting** - Directory name = purpose
4. **Scalable** - Easy to add new configs

---

## ğŸ”„ Data Flow

### Pre-training Pipeline
```
Raw Data â†’ [10_prepare_corpus] â†’ Cleaned Corpus
                                      â†“
Corpus Mixture â†’ [20_train_tokenizer] â†’ SentencePiece Model (32k vocab)
                                              â†“
Corpus + Tokenizer â†’ [30_pack_dataset] â†’ Packed Sequences (1024 tokens)
                                              â†“
Packed Data â†’ [40_pretrain] â†’ Pre-trained Model
                                    â†“
Model + Tokenizer â†’ [50_evaluate_pretrain] â†’ Metrics
```

### SFT Pipeline
```
SFT Datasets â†’ [60_prepare_sft_corpus] â†’ Formatted Conversations
                                              â†“
Conversations + Tokenizer â†’ [60 --enable_packing] â†’ Packed SFT Data
                                                        â†“
Packed Data + Pre-trained Model â†’ [70_train_sft] â†’ LoRA Adapters
                                                        â†“
                                              [70 --merge_adapters]
                                                        â†“
                                                  Merged Model
                                                        â†“
Merged Model â†’ [80_evaluate_sft] â†’ SFT Metrics
```

---

## ğŸ—‚ï¸ File Naming Conventions

### Scripts
```
{stage_number}_{action}_{target}.py

Examples:
- 10_prepare_corpus.py        # Stage 10, prepare corpus
- 70_train_sft.py              # Stage 70, train SFT
- 80_evaluate_sft.py           # Stage 80, evaluate SFT
```

### Configs
```
{category}/{subcategory}/{name}.json

Examples:
- architectures/tiny.json              # Tiny model architecture
- pretrain/sources/c4_english.json     # C4 corpus source
- sft/training/lora_tiny_23m.json      # LoRA config for 23M model
```

### Data Directories
```
data/{stage}/{name}_{vocab}_{seqlen}[_version]

Examples:
- data/corpus/c4/                                   # Stage 1: Cleaned corpus
- data/processed/c4_32k_1024/                       # Stage 3: Tokenized (32k vocab, 1024 seq)
- data/sft_processed/alpaca_instruct_32k_1024_v3/   # SFT packed (v3.0 format)
```

---

## ğŸ¯ Quick Navigation Guide

**I want to...**

| Task | Location |
|------|----------|
| Change model size | `config/architectures/` |
| Add new corpus source | `config/pretrain/sources/` |
| Modify token budget | `config/pretrain/training/` |
| Configure SFT dataset | `config/sft/datasets/` |
| Tune LoRA hyperparams | `config/sft/training/` |
| Run pre-training | `scripts/40_pretrain.py` |
| Run SFT | `scripts/70_train_sft.py` |
| Evaluate model | `scripts/50_evaluate_pretrain.py` or `scripts/80_evaluate_sft.py` |
| Serve model | `scripts/100_serve.py` |
| Debug tokenizer | `utils/debug/tokenizer_metrics.py` |

---

## âœ… Design Principles

This structure follows:

1. **Modularity** - Each script has one clear responsibility
2. **Separation of Concerns** - Config vs code vs data
3. **Discoverability** - Numbered scripts = execution order
4. **Scalability** - Room to add new stages (15, 25, 35...)
5. **Clarity** - Self-documenting names and hierarchy
6. **Best Practices** - Industry-standard organization

---

## ğŸ†š Comparison: Old vs New

### Old Structure (v1.x)
```
âŒ Flat config directory
âŒ Scripts numbered 01-06 (ambiguous)
âŒ ChatML mixed with Instruct
âŒ Utils all in one place
âŒ Cross-imports between scripts
```

### New Structure (v2.0)
```
âœ… Hierarchical config (architectures/pretrain/sft)
âœ… Scripts numbered 10-100 (clear stages)
âœ… Instruct format only (works!)
âœ… Essential vs debug utils separated
âœ… Zero cross-imports (functions in utils/)
```

**Result**: -50% config files, clearer organization, easier maintenance

---

## ğŸ“š Related Documentation

- [README.md](README.md) - Getting started guide
- [MIGRATION.md](MIGRATION.md) - v1â†’v2 migration guide
- [BREAKING_CHANGES.md](BREAKING_CHANGES.md) - Breaking changes list
- [CLEANUP_SUMMARY.md](CLEANUP_SUMMARY.md) - What changed in cleanup
- [TODO_REMAINING.md](TODO_REMAINING.md) - Remaining tasks
- [CLAUDE.md](CLAUDE.md) - Claude Code instructions

---

**Last Updated**: 2025-10-02
**Version**: 2.0.0
