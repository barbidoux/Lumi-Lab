# ğŸš€ ProcÃ©dure ComplÃ¨te : EntraÃ®nement Tokenizer + Dataset Tiny 23M Chinchilla

## ğŸ“‹ Vue d'ensemble

Cette procÃ©dure vous guide pour :
1. **EntraÃ®ner un tokenizer SentencePiece** sur un mix reprÃ©sentatif de 4 sources (7M tokens)
2. **GÃ©nÃ©rer un dataset complet** de 500M tokens pour un modÃ¨le 23M paramÃ¨tres (ratio Chinchilla optimal)

## ğŸ¯ Configuration Cible

- **ModÃ¨le** : 23M paramÃ¨tres
- **Dataset** : 500M tokens (ratio Chinchilla 22:1)
- **Tokenizer** : SentencePiece 32K vocabulaire
- **SÃ©quences** : 1024 tokens
- **Sources** : C4-EN, Gutenberg, FineWeb-Edu, VietGPT-Wikipedia (25% chacune)

---

## ğŸ“ Ã‰TAPE 1 : PrÃ©paration des RÃ©pertoires

```bash
# CrÃ©er la structure de rÃ©pertoires
mkdir -p data/tokenizer_corpus
mkdir -p data/full_dataset_corpus
mkdir -p data/tokenizer
mkdir -p data/packed
mkdir -p logs
```

---

## ğŸ”¤ Ã‰TAPE 2 : EntraÃ®nement du Tokenizer

### 2.1 Analyser les sources pour le tokenizer

```bash
python scripts/01_prepare_corpus.py \
  --config config/tokenizer_training_mix.yaml \
  --output-dir data/tokenizer_corpus \
  --analyze-only \
  --log-level INFO

# âœ… VÃ©rifier les rÃ©sultats dans data/tokenizer_corpus/plan.json
```

### 2.2 PrÃ©parer le corpus pour tokenizer (7M tokens)

```bash
python scripts/01_prepare_corpus.py \
  --config config/tokenizer_training_mix.yaml \
  --output-dir data/tokenizer_corpus \
  --shard-size 5000 \
  --log-level INFO

# â±ï¸ Temps estimÃ©: 15-30 minutes
# âœ… VÃ©rifier: data/tokenizer_corpus/manifest.json et data/tokenizer_corpus/shards/
```

### 2.3 EntraÃ®ner le tokenizer SentencePiece

```bash
python scripts/02_train_tokenizer.py \
  --manifest data/tokenizer_corpus/manifest.json \
  --output-dir data/tokenizer \
  --vocab-size 32768 \
  --model-type unigram \
  --character-coverage 0.9995 \
  --max-training-sentences 2000000 \
  --log-level INFO

# â±ï¸ Temps estimÃ©: 10-20 minutes
# âœ… VÃ©rifier: data/tokenizer/spm.model et data/tokenizer/tokenizer_config.json
```

---

## ğŸ“Š Ã‰TAPE 3 : GÃ©nÃ©ration du Dataset Complet (500M tokens)

### 3.1 Analyser les sources complÃ¨tes

```bash
python scripts/01_prepare_corpus.py \
  --config config/tiny_23M_full_dataset.yaml \
  --output-dir data/full_dataset_corpus \
  --analyze-only \
  --log-level INFO

# âœ… VÃ©rifier le plan dans data/full_dataset_corpus/plan.json
# ğŸ” VÃ©rifier que chaque source planifie ~31M Ã©chantillons pour 125M tokens
```

### 3.2 PrÃ©parer le corpus complet

```bash
python scripts/01_prepare_corpus.py \
  --config config/tiny_23M_full_dataset.yaml \
  --output-dir data/full_dataset_corpus \
  --shard-size 10000 \
  --log-level INFO

# â±ï¸ Temps estimÃ©: 2-4 heures (500M tokens)
# âœ… VÃ©rifier: data/full_dataset_corpus/manifest.json
# ğŸ“ˆ Surveiller les logs pour voir la progression par source
```

### 3.3 Tokeniser et packager les sÃ©quences

```bash
python scripts/03_pack_sequences.py \
  --manifest data/full_dataset_corpus/manifest.json \
  --tokenizer-dir data/tokenizer \
  --output-dir data/packed \
  --sequence-length 1024 \
  --train-val-split 0.98 \
  --log-level INFO

# â±ï¸ Temps estimÃ©: 30-60 minutes
# âœ… VÃ©rifier: data/packed/final_manifest.json, train.bin, val.bin
```

---

## ğŸ” Ã‰TAPE 4 : Validation et VÃ©rification

### 4.1 VÃ©rifier le tokenizer

```bash
# Le tokenizer a Ã©tÃ© testÃ© automatiquement dans l'Ã©tape 2.3
# VÃ©rifier les logs pour s'assurer que les tests ont rÃ©ussi

# Test manuel optionnel:
python -c "
import sentencepiece as spm
sp = smp.SentencePieceProcessor()
sp.load('data/tokenizer/smp.model')
text = 'Hello world! This is a test.'
tokens = sp.encode(text)
decoded = sp.decode(tokens)
print(f'Original: {text}')
print(f'Tokens: {tokens}')
print(f'Decoded: {decoded}')
print(f'Vocab size: {sp.vocab_size()}')
"
```

### 4.2 VÃ©rifier le dataset final

```bash
# VÃ©rifier les statistiques finales
cat data/packed/final_manifest.json | jq '.statistics'

# VÃ©rifications attendues:
# - total_tokens_used: ~500M
# - train_sequences: ~478K (500M / 1024 * 0.98)
# - val_sequences: ~10K (500M / 1024 * 0.02)
# - sequence_length: 1024
# - vocab_size: 32768
```

### 4.3 Test de chargement des donnÃ©es

```bash
python -c "
import numpy as np
import json

# Charger les infos
with open('data/packed/train.idx', 'r') as f:
    train_info = json.load(f)

# Charger les donnÃ©es via memmap
train_data = np.memmap(
    'data/packed/train.bin',
    dtype=np.uint16,
    mode='r',
    shape=tuple(train_info['shape'])
)

print(f'Dataset shape: {train_data.shape}')
print(f'Sample sequence: {train_data[0][:10].tolist()}')
print(f'Min token ID: {train_data.min()}')
print(f'Max token ID: {train_data.max()}')
"
```

---

## ğŸ“ˆ Ã‰TAPE 5 : Monitoring et Logs

### Fichiers de logs Ã  surveiller :
- `logs/01_prepare_corpus.log` - Progression du traitement
- `logs/02_train_tokenizer.log` - EntraÃ®nement tokenizer
- `logs/03_pack_sequences.log` - Packaging des sÃ©quences

### MÃ©triques clÃ©s Ã  vÃ©rifier :
- **Tokenizer** : 7M tokens d'entraÃ®nement, 32K vocab
- **Corpus** : 500M tokens, ~488K sÃ©quences de 1024 tokens
- **Distribution** : 25% par source (125M tokens chacune)
- **QualitÃ©** : Taux de dÃ©duplication, filtrage linguistique

---

## âœ… Ã‰TAPE 6 : RÃ©sultats Attendus

Ã€ la fin de cette procÃ©dure, vous devriez avoir :

```
data/
â”œâ”€â”€ tokenizer/
â”‚   â”œâ”€â”€ spm.model                    # ModÃ¨le SentencePiece
â”‚   â”œâ”€â”€ spm.vocab                    # Vocabulaire
â”‚   â””â”€â”€ tokenizer_config.json        # Configuration
â”œâ”€â”€ packed/
â”‚   â”œâ”€â”€ train.bin                    # DonnÃ©es d'entraÃ®nement (478K sÃ©quences)
â”‚   â”œâ”€â”€ train.idx                    # Index d'entraÃ®nement
â”‚   â”œâ”€â”€ val.bin                      # DonnÃ©es de validation (10K sÃ©quences)
â”‚   â”œâ”€â”€ val.idx                      # Index de validation
â”‚   â””â”€â”€ final_manifest.json          # Manifest final
â””â”€â”€ [corpus directories avec shards compressÃ©s]
```

### Statistiques finales attendues :
- **Total tokens** : 500M (Chinchilla-optimal pour 23M paramÃ¨tres)
- **SÃ©quences d'entraÃ®nement** : ~478,000 (98%)
- **SÃ©quences de validation** : ~10,000 (2%)
- **Taille sur disque** : ~2GB (format binaire compressÃ©)

---

## ğŸš¨ DÃ©pannage

### ProblÃ¨mes courants :

1. **Erreur d'authentification HF** :
   ```bash
   export HF_TOKEN="your_token_here"
   # ou utilisez huggingface-cli login
   ```

2. **MÃ©moire insuffisante** :
   - RÃ©duire `shard_size` Ã  5000
   - Traiter les sources une par une

3. **Erreur de tokenizer** :
   - VÃ©rifier que spm.model existe
   - Relancer l'Ã©tape 2.3 si nÃ©cessaire

4. **DonnÃ©es corrompues** :
   - Utiliser `--force` pour regÃ©nÃ©rer
   - VÃ©rifier l'espace disque disponible

---

## ğŸ“ Commandes de Diagnostic

```bash
# VÃ©rifier l'espace disque
df -h

# VÃ©rifier les processus Python
ps aux | grep python

# Taille des fichiers gÃ©nÃ©rÃ©s
du -sh data/*/

# VÃ©rifier l'intÃ©gritÃ© des manifests
python -m json.tool data/tokenizer_corpus/manifest.json
python -m json.tool data/full_dataset_corpus/manifest.json
python -m json.tool data/packed/final_manifest.json
```

Votre dataset sera prÃªt pour l'entraÃ®nement du modÃ¨le 23M paramÃ¨tres ! ğŸ¯